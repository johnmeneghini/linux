# FPIN LI Support Changes: From fpin_v9 to fpin_v10

This document analyzes the difference between the `fpin_v9` and `fpin_v10` changes to implement improved FPIN (Fabric Performance Impact Notification) Link Integrity support for NVMe over Fibre Channel.

## Summary

The changes represent a significant architectural improvement in how FPIN Link Integrity events are processed for NVMe targets. The key improvement is moving FPIN processing from the NVMe FC layer to the SCSI FC transport layer, providing a more unified and maintainable approach.

## Key Architectural Changes

### 1. **Processing Location Migration**
- **Before**: FPIN processing was handled directly in the NVMe FC driver
- **After**: FPIN processing moved to SCSI FC transport layer, with callback into NVMe layer

### 2. **API Simplification**
- **Before**: Complex `nvme_fc_fpin_rcv()` function that parsed FPIN messages
- **After**: Simple `nvme_fc_modify_rport_fpin_state()` function with clean interface

### 3. **Thread Safety Improvements**
- **Before**: Missing locking in `nvme_fc_rport_from_wwpn()` and `fc_rport_set_marginal_state()`
- **After**: Proper locking added to prevent race conditions

### 4. **Enhanced Sysfs Integration**
- **Before**: Basic `fc_rport_set_marginal_state()` with no NVMe integration
- **After**: Full SCSI-NVMe state synchronization with proper locking

## Detailed Changes by File

### drivers/nvme/host/fc.c

#### ğŸ”§ **Thread Safety Fix**
```c
// BEFORE: No locking around list traversal
static struct nvme_fc_rport *nvme_fc_rport_from_wwpn(struct nvme_fc_lport *lport, u64 rport_wwpn)
{
    struct nvme_fc_rport *rport;

    list_for_each_entry(rport, &lport->endp_list, endp_list) {
        // ... unsafe list traversal
    }
}

// AFTER: Proper locking added
static struct nvme_fc_rport *nvme_fc_rport_from_wwpn(struct nvme_fc_lport *lport, u64 rport_wwpn)
{
    struct nvme_fc_rport *rport;
    unsigned long flags;

    spin_lock_irqsave(&nvme_fc_lock, flags);
    list_for_each_entry(rport, &lport->endp_list, endp_list) {
        // ... safe list traversal with proper locking
        if (match_found) {
            spin_unlock_irqrestore(&nvme_fc_lock, flags);
            return rport;
        }
    }
    spin_unlock_irqrestore(&nvme_fc_lock, flags);
}
```

#### â• **New Functions Added**
1. **`nvme_fc_lport_from_wwpn()`** - Find local port by WWPN
   - Thread-safe lookup using `nvme_fc_lock`
   - Proper reference counting with `nvme_fc_lport_get()`

2. **`nvme_fc_fpin_set_state()`** - Set marginal state on controllers
   - Simplified logic compared to old `nvme_fc_fpin_li_lport_update()`
   - Supports both setting and clearing marginal state
   - Proper locking with `rport->lock`

3. **`nvme_fc_modify_rport_fpin_state()`** - Main API function
   - Clean interface: takes WWPNs and marginal flag
   - Exported with `EXPORT_SYMBOL_GPL`
   - Replaces complex `nvme_fc_fpin_rcv()` function

#### âŒ **Functions Removed**
1. **`nvme_fc_fpin_li_lport_update()`** - Complex FPIN processing logic
2. **`nvme_fc_fpin_rcv()`** - Direct FPIN message processing

### drivers/scsi/scsi_transport_fc.c

#### â• **New FPIN Processing Function**
**`fc_host_fpin_set_nvme_rport_marginal()`**
- Parses FPIN Link Integrity descriptors
- Identifies affected NVMe target ports
- Sets `FC_PORTSTATE_MARGINAL` on SCSI rports
- Calls into NVMe layer via `nvme_fc_modify_rport_fpin_state()`
- Proper locking with `shost->host_lock`

Key features:
```c
/* Parse FPIN descriptors */
while (bytes_remain >= FC_TLV_DESC_HDR_SZ) {
    switch (dtag) {
    case ELS_DTAG_LNK_INTEGRITY:
        // Process Link Integrity descriptor
        for (i = 0; i < pname_count; i++) {
            wwpn = be64_to_cpu(li_desc->pname_list[i]);
            rport = fc_find_rport_by_wwpn(shost, wwpn);

            if (rport && rport->roles & FC_PORT_ROLE_NVME_TARGET) {
                rport->port_state = FC_PORTSTATE_MARGINAL;
                // Call into NVMe layer
                nvme_fc_modify_rport_fpin_state(local_wwpn, wwpn, true);
            }
        }
        break;
    }
}
```

#### ğŸ”§ **Enhancement: fc_rport_set_marginal_state() Function**

This function represents one of the most significant improvements in the fpin_v10 changes. It provides a sysfs interface for manually controlling remote port marginal states, with full integration into the NVMe layer.

**Call Graph:**
```
/sys/class/fc_remote_ports/rport-X:Y-Z/port_state (write)
    â†“
fc_rport_set_marginal_state(dev, attr, buf, count)
    â†“
get_fc_port_state_match(buf, &port_state)              // Parse user input
    â†“
spin_lock_irqsave(shost->host_lock, flags)             // Thread safety
    â†“
switch (port_state) {
    case FC_PORTSTATE_MARGINAL:
        rport->port_state = FC_PORTSTATE_MARGINAL       // Update SCSI state
        â†“
        spin_unlock_irqrestore(shost->host_lock, flags)
        â†“
        nvme_fc_modify_rport_fpin_state(local_wwpn, remote_wwpn, true)
            â†“
            nvme_fc_lport_from_wwpn(local_wwpn)         // Find NVMe local port
            â†“
            nvme_fc_fpin_set_state(lport, remote_wwpn, true)
                â†“
                nvme_fc_rport_from_wwpn(lport, remote_wwpn)  // Find NVMe remote port
                â†“
                spin_lock_irq(&rport->lock)             // NVMe-level locking
                â†“
                set_bit(NVME_CTRL_MARGINAL, &ctrl->ctrl.flags)  // Set marginal on all controllers
                â†“
                spin_unlock_irq(&rport->lock)
                â†“
                nvme_fc_rport_put(rport)                // Release reference
            â†“
            nvme_fc_lport_put(lport)                    // Release reference

    case FC_PORTSTATE_ONLINE: [Similar flow but clear_bit()]
}
```

**Before/After Comparison:**

**fpin_v9 version** (Basic and Unsafe):
```c
static ssize_t fc_rport_set_marginal_state(struct device *dev,
                                           struct device_attribute *attr,
                                           const char *buf, size_t count)
{
    struct fc_rport *rport = transport_class_to_rport(dev);
    enum fc_port_state port_state;
    int ret = 0;

    ret = get_fc_port_state_match(buf, &port_state);
    if (ret)
        return -EINVAL;

    // âŒ NO LOCKING - Race condition prone!
    if (port_state == FC_PORTSTATE_MARGINAL) {
        if (rport->port_state == FC_PORTSTATE_ONLINE)
            rport->port_state = port_state;  // âŒ Direct assignment without protection
        else if (port_state != rport->port_state)
            return -EINVAL;
    } else if (port_state == FC_PORTSTATE_ONLINE) {
        if (rport->port_state == FC_PORTSTATE_MARGINAL)
            rport->port_state = port_state;  // âŒ Direct assignment without protection
        else if (port_state != rport->port_state)
            return -EINVAL;
    } else
        return -EINVAL;

    // âŒ NO NVME INTEGRATION - SCSI and NVMe states become inconsistent!
    return count;
}
```

**fpin_v10 version** (Robust and Integrated):
```c
static ssize_t fc_rport_set_marginal_state(struct device *dev,
                                           struct device_attribute *attr,
                                           const char *buf, size_t count)
{
    struct fc_rport *rport = transport_class_to_rport(dev);
    struct Scsi_Host *shost = rport_to_shost(rport);      // âœ… Get host for WWPN
    u64 local_wwpn = fc_host_port_name(shost);            // âœ… Extract local WWPN
    enum fc_port_state port_state;
    int ret = 0;
    unsigned long flags;                                   // âœ… For proper locking

    ret = get_fc_port_state_match(buf, &port_state);
    if (ret)
        return -EINVAL;

    spin_lock_irqsave(shost->host_lock, flags);           // âœ… PROPER LOCKING

    switch (port_state) {                                 // âœ… Cleaner structure
    case FC_PORTSTATE_MARGINAL:
        if (rport->port_state == FC_PORTSTATE_ONLINE) {
            rport->port_state = port_state;               // âœ… Protected assignment
            spin_unlock_irqrestore(shost->host_lock, flags);
#if (IS_ENABLED(CONFIG_NVME_FC))
            nvme_fc_modify_rport_fpin_state(local_wwpn,   // âœ… NVME INTEGRATION
                            rport->port_name, true);
#endif
            return count;                                 // âœ… Early return on success
        }
        break;

    case FC_PORTSTATE_ONLINE:
        if (rport->port_state == FC_PORTSTATE_MARGINAL) {
            rport->port_state = port_state;               // âœ… Protected assignment
            spin_unlock_irqrestore(shost->host_lock, flags);
#if (IS_ENABLED(CONFIG_NVME_FC))
            nvme_fc_modify_rport_fpin_state(local_wwpn,   // âœ… NVME INTEGRATION
                            rport->port_name, false);
#endif
            return count;                                 // âœ… Early return on success
        }
        break;
    default:
        break;
    }

    // âœ… Unified error handling
    if (port_state != rport->port_state) {
        spin_unlock_irqrestore(shost->host_lock, flags);
        return -EINVAL;
    }

    spin_unlock_irqrestore(shost->host_lock, flags);
    return count;
}
```

**Key Improvements:**

1. **ğŸ”— NVMe Integration**
   - Calls `nvme_fc_modify_rport_fpin_state()` to sync SCSI and NVMe states
   - Ensures both SCSI rport and NVMe controller reflect the same marginal state
   - Prevents inconsistency between FC layers

2. **ğŸ—ï¸ Better Code Structure**
   - Switch statement instead of complex if-else chains
   - Early returns for success cases reduce nesting
   - Unified error handling path

3. **ğŸ“Š Enhanced State Management**
   - Bidirectional state transitions: Online â†” Marginal
   - Both directions properly integrated with NVMe layer
   - Local WWPN extraction for proper NVMe port identification

4. **ğŸ›¡ï¸ Robustness**
   - Proper validation of state transitions
   - Lock held for minimal duration (released before NVMe calls)
   - Comprehensive error handling

**Impact:**
This enhancement enables administrators to manually control port marginal states via sysfs:
```bash
# Set port to marginal state
echo "Marginal" > /sys/class/fc_remote_ports/rport-4:0-1/port_state

# Clear marginal state (set to online)
echo "Online" > /sys/class/fc_remote_ports/rport-4:0-1/port_state
```

Both operations now properly synchronize the state across SCSI FC transport and NVMe FC layers, providing consistent behavior throughout the storage stack.

### include/linux/nvme-fc-driver.h

#### ğŸ”„ **API Changes**
```c
// BEFORE: Complex FPIN message processing
void nvme_fc_fpin_rcv(struct nvme_fc_local_port *localport,
                     u32 fpin_len, char *fpin_buf);

// AFTER: Simple state modification interface
void nvme_fc_modify_rport_fpin_state(u64 local_wwpn, u64 remote_wwpn, bool marginal);
```

Benefits of new API:
- **Simpler**: No need to parse FPIN messages in NVMe layer
- **Cleaner**: Clear parameters indicating what action to take
- **More flexible**: Can be called from various contexts (FPIN, sysfs, etc.)

### include/scsi/scsi_transport_fc.h

#### â• **New Export Added**
```c
void fc_host_fpin_set_nvme_rport_marginal(struct Scsi_Host *shost, u32 fpin_len, char *fpin_buf);
```

### Driver Integration Changes

#### drivers/scsi/lpfc/lpfc_els.c
```c
// BEFORE: Direct call to NVMe layer
#if (IS_ENABLED(CONFIG_NVME_FC))
    if (vport->cfg_enable_fc4_type & LPFC_ENABLE_NVME)
        nvme_fc_fpin_rcv(vport->localport, fpin_length, (char *)fpin);
#endif

// AFTER: Call through SCSI transport layer
if (vport->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {
    fc_host_fpin_set_nvme_rport_marginal(lpfc_shost_from_vport(vport),
                                         fpin_length, (char *)fpin);
}
```

#### drivers/scsi/qla2xxx/qla_isr.c
```c
// BEFORE: Direct NVMe call
#if (IS_ENABLED(CONFIG_NVME_FC))
    nvme_fc_fpin_rcv(vha->nvme_local_port, pkt_size, (char *)pkt);
#endif

// AFTER: SCSI transport layer call
fc_host_fpin_set_nvme_rport_marginal(vha->host, pkt_size, (char *)pkt);
```

#### drivers/scsi/qla2xxx/qla_os.c
Minor cleanup in `qla24xx_free_purex_item()` function for better code organization.

## Benefits of the Changes

### 1. **Improved Architecture** ğŸ—ï¸
- **Separation of Concerns**: FPIN parsing in SCSI layer, state management in NVMe layer
- **Code Reuse**: FPIN parsing logic shared across all FC drivers
- **Maintainability**: Changes to FPIN processing only need to be made in one place

### 2. **Thread Safety** ğŸ”’
- Proper locking throughout the call chain
- Reference counting prevents use-after-free scenarios

### 3. **API Simplification** ğŸ“‹
- Clean, simple interface for NVMe state modification
- Reduced complexity in NVMe FC driver
- Better testability and debugging

### 4. **Enhanced Functionality** âš¡
- Support for both setting and clearing marginal state
- Integration with sysfs interface for manual control
- Better error handling and validation

### 5. **Driver Consistency** ğŸ”„
- Consistent FPIN handling across LPFC and QLA drivers
- Unified approach eliminates driver-specific variations
- Easier to add support for new drivers

## Call Flow Comparison

### Before (fpin_v9)

**FPIN Processing Path:**
```
LPFC/QLA Driver
    â†“
nvme_fc_fpin_rcv()
    â†“
nvme_fc_fpin_li_lport_update()
    â†“
[Complex FPIN parsing and processing]
    â†“
Set NVME_CTRL_MARGINAL flags
```

**Sysfs Path (Limited):**
```
/sys/.../port_state (write)
    â†“
fc_rport_set_marginal_state()
    â†“
[Basic state change - NO locking]
    â†“
rport->port_state = new_state
[NO NVMe integration - states become inconsistent!]
```

### After (fpin_v10)

**FPIN Processing Path:**
```
LPFC/QLA Driver
    â†“
fc_host_fpin_set_nvme_rport_marginal()
    â†“
[FPIN parsing in SCSI transport layer]
    â†“
Set FC_PORTSTATE_MARGINAL on SCSI rport
    â†“
nvme_fc_modify_rport_fpin_state()
    â†“
nvme_fc_lport_from_wwpn()
    â†“
nvme_fc_fpin_set_state()
    â†“
Set/Clear NVME_CTRL_MARGINAL flags
```

**Enhanced Sysfs Path (Fully Integrated):**
```
/sys/.../port_state (write)
    â†“
fc_rport_set_marginal_state()
    â†“
spin_lock_irqsave(shost->host_lock)  [PROPER LOCKING]
    â†“
rport->port_state = new_state
    â†“
spin_unlock_irqrestore(shost->host_lock)
    â†“
nvme_fc_modify_rport_fpin_state()    [NVME INTEGRATION]
    â†“
nvme_fc_lport_from_wwpn()
    â†“
nvme_fc_fpin_set_state()
    â†“
Set/Clear NVME_CTRL_MARGINAL flags
[SCSI and NVMe states now fully synchronized!]
```

## Testing and Integration Points

### 1. **FPIN Reception Path**
- LPFC and QLA drivers receive FPIN messages
- SCSI transport layer processes and parses FPIN descriptors
- NVMe layer updates controller marginal state

### 2. **Enhanced Sysfs Interface**
- Manual control via `/sys/class/fc_remote_ports/*/port_state`
- **NEW**: Full SCSI-NVMe state synchronization
- Supports both Online â†” Marginal state transitions
- **NEW**: Consistent behavior across all FC protocol layers
- **NEW**: Administrative control with immediate NVMe controller impact

### 3. **Error Recovery**
- Proper cleanup of references in error paths
- Thread-safe operations under concurrent access
- Graceful handling of non-existent ports/controllers

## Conclusion

The changes from `fpin_v9` to `fpin_v10` represent an **improvement** in the FPIN Link Integrity support for NVMe over Fibre Channel. The new architecture provides:

- **Better separation of concerns** between SCSI and NVMe layers
- **Simplified API** that's easier to use and maintain
- **Enhanced functionality** supporting both directions of state changes
- **Unified approach** across different FC HBA drivers
- **ğŸ†• Sysfs integration** - the `fc_rport_set_marginal_state()` enhancement represents one of the most significant improvements, providing:
  - Thread-safe manual port state control
  - Complete SCSI-NVMe layer synchronization
  - Administrative control over multipath behavior
  - Consistent state management across all FC protocol layers

### Key Changes:

1. **ğŸ”’ Thread Safety**: Fixed critical race conditions that could cause system instability
2. **ğŸ”— Layer Integration**: SCSI and NVMe states stay perfectly synchronized
3. **ğŸ› ï¸ Administrative Control**: Operations teams can now reliably manage port states
4. **ğŸ“Š Consistent Behavior**: No more state mismatches between FC protocol layers
5. **ğŸ—ï¸ Maintainability**: Cleaner architecture that's easier to debug and enhance

